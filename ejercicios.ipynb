{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios\n",
        "\n",
        "Esta sesión consta de un ejercicio mínimo y de varias ampliaciones que no son obligatorias.\n",
        "\n",
        "## Ejercicio mínimo (5 puntos)\n",
        "\n",
        "Usando el notebook de explicación construye un modelo para el dataset de neutralización y súbelo a HuggingFace. Incluye en la siguiente celda el enlace a tu modelo en HuggingFace y guarda los cambios en GitHub.  \n"
      ],
      "metadata": {
        "id": "jOf0qe-9cyx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mínimo - Enlace al modelo:  https://huggingface.co/HektorOrion/mbart-neutralization\n",
        "\n",
        "Pongo los otros enlaces aquí por si hay problemas con el guardado en el repositorio:\n",
        "Ej.1 -Enlace al dataset ADV-ELE: https://huggingface.co/datasets/HektorOrion/adv-ele\n",
        "\n",
        "Ej.2 - Modelo distinto: https://github.com/masterPLIA2324/traduccion-automatica-sesion-3-HectorSamadi/blob/main/AT3_NuevoModelo.ipynb (da error)\n"
      ],
      "metadata": {
        "id": "mz7oJCDXdgeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicios adicionales\n",
        "\n",
        "1. Utiliza un dataset de traducción distinto al de neutralización (2 puntos).\n",
        "2. Utiliza un modelo distinto al de mbart (puedes usar modelos como t5 o pegasus) (2 puntos).\n",
        "3. Usando lo que veremos en la última sesión del curso, construye un dataset para traducción y construye un modelo que lo utilice (1 punto).\n",
        "\n",
        "Si haces estas tareas debes crear un nuevo notebook de Colaboratory (una para cada una de ellas) y guardarlo en tu repositorio de GitHub."
      ],
      "metadata": {
        "id": "epMKT8IGdmCb"
      }
    }
  ]
}